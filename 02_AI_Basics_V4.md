
02_AI_Basics.md
<!-- author: AI in TVET Workshop Team version: 1.0.0 language: en narrator: US English Female comment: AI Basics for TVET Teachers (Human Agency Competency) link: https://raw.githubusercontent.com/OVGU-VET-TechEd/ASSET_UNESCO_Coinitiative/refs/heads/main/ASSET_basic.css @style .sector-card { background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 2rem; margin: 1rem; border-radius: 15px; box-shadow: 0 8px 32px rgba(0,0,0,0.3); transition: transform 0.3s ease; } .sector-card:hover { transform: translateY(-5px); } .ai-tool-demo { background: #f8f9fa; border: 2px solid #007bff; border-radius: 10px; padding: 1.5rem; margin: 1rem 0; } .quiz-interactive { background: linear-gradient(45deg, #ff6b6b, #ffa726); color: white; padding: 1rem; border-radius: 10px; margin: 1rem 0; } .resource-link { background: #28a745; color: white; padding: 0.5rem 1rem; border-radius: 5px; text-decoration: none; display: inline-block; margin: 0.25rem; transition: all 0.3s ease; } .resource-link:hover { background: #218838; transform: scale(1.05); } @end @customQuiz [[.]] <script> "@0" == btoa( "@input".trim().toLowerCase() ) </script> @end @aiDemo: <div class="ai-tool-demo">**AI Demo:** @0<br>**Tool:** @1<br>**Try it:** [Click here](@2)</div> @sectorCard: <div class="sector-card">**@0**<br>@1</div> @resourceLink: <a href="@1" class="resource-link" target="_blank">@0</a> -->
AI Basics: Building Understanding and Human Agency
<div style="text-align:center;"> <img src="https://github.com/OVGU-VET-TechEd/ASSET_UNESCO_Coinitiative/blob/main/media/UNESCO-UNEVOC_logo.png?raw=true" alt="UNEVOC Logo" width="80" style="margin:0 10px;"/> <img src="https://github.com/OVGU-VET-TechEd/ASSET_UNESCO_Coinitiative/blob/main/media/ASSET_icon.png?raw=true" alt="ASSET Logo" width="80" style="margin:0 10px;"/> <img src="https://github.com/OVGU-VET-TechEd/ASSET_UNESCO_Coinitiative/blob/main/media/hrdc_logo.png?raw=true" alt="HRDC Logo" width="80" style="margin:0 10px;"/> <img src="https://github.com/OVGU-VET-TechEd/ASSET_UNESCO_Coinitiative/blob/main/media/mitd_logo.png?raw=true" alt="MITD Logo" width="80" style="margin:0 10px;"/> <img src="https://github.com/OVGU-VET-TechEd/ASSET_UNESCO_Coinitiative/blob/main/media/HWK_Blume.png?raw=true" alt="HWK Blume Logo" width="80" style="margin:0 10px;"/> </div>
Introduction
In this module, we will establish clear definitions of key AI concepts and explore why human agency is paramount – meaning that humans (teachers and students) stay in control of AI, rather than being controlled by it. Rani, our electronics teacher, has started learning about AI. Now she wants to understand the technical basics: What exactly are these “models” and “algorithms” she keeps hearing about? Why do people mention AI “hallucinating” or being “biased”? We’ll unpack these terms with simple explanations and practical examples from the classroom. By the end, you should feel comfortable with the vocabulary of AI and recognize how to maintain human oversight when using AI in education.
Key Definitions: AI Jargon Demystified
Let’s define some fundamental terms in simple language:
AI (Artificial Intelligence): The broad field of creating machines or software that can perform tasks requiring human-like intelligence. This ranges from simple algorithms recommending what video to watch next, to complex systems like self-driving car software. In essence, AI tries to mimic cognitive functions such as learning and problem-solving. (Example: A tutoring app that adapts to a student’s answers is using AI to “intelligently” personalize practice questions.)
Algorithm: A step-by-step procedure or set of rules for solving a problem. In computing, an algorithm is like a recipe that tells the computer what steps to take. Traditional algorithms are explicitly programmed by humans. In AI, algorithms often improve themselves by learning from data. (Example: A sorting algorithm arranges student names alphabetically. An AI algorithm might learn to sort images of components by type after being trained on labeled examples.)
Model: In AI, a “model” usually means the trained program that can make predictions or decisions. It’s the result of a learning process. Think of a model as a function or formula that has been tuned based on data. For instance, after feeding an AI many circuit diagrams and their fault diagnoses, you get a model that can predict faults in a new circuit diagram. The model is defined by its weights (see below) which encode the patterns it learned.
Weights: These are the internal parameters of an AI model that get adjusted during training. If the AI model is like a brain, the weights are like the strengths of the connections between neurons. They determine how input is transformed into output. When we train a model (say, to recognize faulty circuits), we are essentially tweaking the weights so that the model’s predictions match the correct answers in our training data. (You won’t typically see weights, but they’re the reason a model behaves as it does.)
Bias (in AI models): This can mean two things. One is technical bias – e.g., a model’s bias term, which is an offset added in calculations. More importantly for us, bias refers to systematic errors in the AI’s output that discriminate or favor one outcome unfairly. This happens if the training data or algorithm design has skewed patterns. For example, if an AI teaching assistant was trained mostly on questions from one region, it might be biased and not understand contexts from another region. Bias in AI can lead to unfair or misleading results, so we must be vigilant.
Prompting: Communicating with an AI by giving it an input or question (a prompt). With modern AI like chatbots and large language models, prompting is how you “program” the AI in plain language. The quality of the AI’s response depends on how well you phrase your prompt. Prompting involves giving clear instructions, context, or even examples to guide the AI’s output. (We’ll cover specific prompting strategies in a later module, but essentially, learning to prompt is like learning to ask questions effectively.)
Now, let’s make sure we truly understand these. Here’s an interactive match of terms to simple explanations:
<details><summary><strong>AI</strong></summary>Machines performing intelligent tasks (learning, decision-making) that typically require human intelligence.</details> <details><summary><strong>Algorithm</strong></summary>A set of step-by-step instructions for solving a problem or performing a task (like a recipe for the computer).</details> <details><summary><strong>Model</strong></summary>An AI program that has been trained on data and can make predictions or decisions (the end product of AI training).</details> <details><summary><strong>Weights</strong></summary>Internal numbers in a model that the training process adjusts. They determine how the model makes predictions (its “knowledge”).</details> <details><summary><strong>Bias</strong></summary>A systematic error or unfairness in AI outputs, often caused by biased training data or assumptions, leading to prejudice in results.</details> <details><summary><strong>Prompt</strong></summary>The input given to an AI system to get a result. Good prompting means giving clear, detailed instructions or questions to guide the AI’s answer.</details>
(Click each term to reveal the definition.) These definitions highlight that AI systems learn from data and can exhibit unexpected behaviors if we’re not careful. Understanding these concepts helps teachers maintain human agency – because if you know how the “AI brain” works at a high level, you can better guide and control it.
Types of AI Algorithms and Systems
Not all AI is the same. Different tasks use different approaches. Here are a few major types of AI algorithms and systems you might hear about:
Rule-based systems: The earliest form of AI – here, human experts define rules (“if-then” logic). For example, a simple tutoring program might have a rule “IF answer is wrong AND common error is X, THEN display hint Y.” These don’t learn on their own; they follow the rules given. They’re transparent and under human control, but not adaptable.
Machine Learning (ML): Instead of relying only on fixed rules, ML algorithms learn from examples. You feed them data, and they find patterns. Within ML:
Supervised Learning: The algorithm learns from labeled examples (we give the right answers during training). E.g., training a model with labeled images of correct vs incorrect solder joints, so it can later classify new images.
Unsupervised Learning: The algorithm finds patterns in unlabeled data on its own (like clustering similar student errors without being told which are which).
Reinforcement Learning: The algorithm learns by trial and error, receiving rewards or penalties (imagine an AI figuring out how to adjust a machine by getting a “score” for performance).
Neural Networks: A subset of ML, inspired by the human brain’s network of neurons. Neural networks consist of layers of interconnected “neurons” (actually just math functions) that can learn very complex patterns. Deep Learning refers to neural networks with many layers (“deep” networks). These are behind much of the recent AI breakthroughs. For instance, a neural network can learn to translate languages or detect objects in images after sufficient training. For Rani, a deep learning model might, say, analyze the sound of a motor and predict if it’s likely to fail soon (because it learned the subtle patterns of sound frequencies that indicate a problem).
Generative AI: These are models designed to generate new content. Examples include large language models (like ChatGPT) which generate text, or image generators like DALL-E. They produce something (essay, image, audio) based on the patterns they learned. Generative models are impressive (they can write answers, create designs, etc.), but they also can hallucinate (produce incorrect or nonsensical output – we’ll discuss this shortly).
Expert Systems: AI systems built to emulate the decision-making of a human expert in a specific field. They often combine rule-based logic with databases of knowledge. In the 1980s-90s, these were common – e.g., an expert system for medical diagnosis or, say, for troubleshooting an electronic device. They require extensive knowledge engineering and don’t learn new rules by themselves, but can be very accurate within their narrow domain if well-crafted.
As a teacher, you don’t need to know the math behind these, but it helps to know what’s out there. For example, if someone says “our school is using a neural network to predict student dropouts,” you know it’s an ML system that was trained on data about students. Or if an app claims “we have an expert system for career guidance,” you know it’s based on predefined knowledge and rules from career counselors. Different AI types have different pros/cons: some are more interpretable (rule-based systems – you can follow the logic), others are more accurate with complexity but opaque (deep neural nets – huge accuracy in pattern recognition, but you can’t easily see why they made a given decision).
Understanding the type of AI helps you anticipate its behavior and limitations. For instance, generative AI might create amazingly human-like text, but it might also confidently state false information (a byproduct of how it’s trained to predict likely words, not verify facts). As teachers, this awareness lets us apply AI appropriately and maintain authority over the final decisions.
Hallucination and Bias: AI’s Quirks and Risks
Two terms that often come up when using modern AI tools are hallucination and bias. These represent ways AI can go wrong. Let’s break them down with examples relevant to teaching:
AI Hallucination: This is when an AI generates an answer that is factually incorrect or nonsensical, but it may sound confident or plausible. The AI isn’t “lying” on purpose; it’s usually a side effect of how generative models work – they predict likely sentences and sometimes stitch together things that seem real but aren’t. For example, Rani once asked a chatbot for a brief bio of a famous electronics engineer. The AI confidently wrote a paragraph with some true facts… and one completely made-up award that engineer supposedly won. The award didn’t exist! The AI “hallucinated” that detail. In a more dangerous case, a student might ask an AI to explain a circuit concept and the AI could invent a fake formula or explanation. If the student or teacher doesn’t catch it, misinformation spreads. Key point: Always verify AI outputs, especially factual ones. As a teacher, double-check any content an AI provides, just like you’d verify a student’s research from Wikipedia or the internet.
AI Bias: We touched on bias in definitions. Bias in AI is when the system’s outputs unfairly favor or disfavor certain groups or outcomes. This often originates from biased training data. For instance, suppose an AI grading assistant was trained mostly on essays written in English by native speakers and it learned to give lower scores to essays with non-native phrasing. If Rani uses this AI on her students, it might systematically give lower marks to students who are ESL (English as a Second Language), not because their content is worse, but because the AI has a bias toward a certain writing style. That would be unfair and reinforce existing inequalities – a clear ethical problem. Another example: an AI career guidance tool might (due to biased historical data) suggest electronics engineering careers more often to male students than female students, thus perpetuating gender stereotypes. Key point: Teachers must be alert to these biases. We should ask: “How was this AI trained? Could that have left out certain populations or perspectives?” and then monitor outputs for signs of bias. If found, we either avoid using that AI or put in safeguards (like telling students about the potential bias or using additional unbiased materials).
Why Human Agency Matters
“Human agency” means humans keep the decision-making power. AI hallucinations and biases illustrate why this is critical. As a teacher, you decide how to use AI output. You don’t blindly accept what the AI says. Human agency involves:
Critical Oversight: Rani uses AI to draft quiz questions, but she reviews each question before giving it to students, to ensure accuracy and appropriateness.
Final Decision Authority: If an AI suggests a grade or feedback, Rani treats it as a suggestion, not the final verdict. She moderates the grade using her professional judgment.
Accountability: Remember that an AI won’t take responsibility if something goes wrong – the teacher or institution will. So Rani remains accountable for any AI-assisted content she shares. This mindset helps her stay in control and not become over-reliant on automation.
Now, let’s illustrate these concepts with a scenario that highlights the risks of bias and hallucination in a classroom decision-making context:
@sectorCard("AI Grading Gone Wrong","Rani experimented with an AI tool that automatically graded student essays about recent advancements in electronics. Initially, it seemed like a time-saver. However, she noticed something troubling: essays written by students whose first language wasn’t English consistently got lower scores, even when their technical content was solid. Upon investigation, Rani realized the AI’s training data didn’t include many examples of non-native English writing, so it had learned a bias for a certain style. In one extreme case, the AI hallucinated feedback, telling a student their essay lacked a section that was actually there! This was a turning point – Rani understood that without her active oversight, the AI could mislead and unfairly judge her students. She decided to stop using the auto-grader and instead use AI in a more controlled way (like generating a draft rubric that she then fine-tunes).")
In the scenario above, Rani’s awareness and human agency prevented potential harm. The lesson: AI can assist, but not replace, the nuanced judgment of a teacher. By understanding AI’s quirks, you can use it as a helpful assistant while shielding your students from its mistakes.
Practical Classroom Example: Bias/Hallucination in Action
Let’s walk through a concrete example to solidify these concepts. Imagine Rani uses an AI-powered quiz generator for a unit test. She inputs “Create a question to test understanding of Ohm’s Law.” The AI outputs: “What is Ohm’s Law? (a) Voltage = Current / Resistance (b) Voltage = Current × Resistance (c) Voltage = Resistance / Current (d) Voltage = Current + Resistance.”
At first glance, option (b) is the correct formula (V = I × R). However, Rani spots a problem – the phrasing of the question is literally just “What is Ohm’s Law?” with formulas as options. One option (d) is nonsensical (you wouldn’t add current and resistance). The AI did the task, but not perfectly: it gave a trivial recall question with one silly distractor. If Rani had blindly used it, students might actually get confused why (d) is even there. This is a mild example of hallucination (the AI generating a bad option) and also a missed opportunity for a better question. Rani intervenes: she edits the question to “Which equation represents Ohm’s Law?” and replaces the silly option with a more plausible mistake (e.g., “Voltage = Current – Resistance”) to truly test understanding. This way, the AI’s output was just a starting point – human insight made it a quality item.
For bias, consider if Rani asked the AI, “Generate a short context for a circuit problem involving a student.” If the AI always names the student “John” who loves electronics, there might be an implicit bias (perhaps all examples in training data featured male students). Rani notices this pattern and starts prompting the AI with diverse names or explicitly telling it to vary contexts (e.g., “use a female student in the problem”). This ensures the materials she generates are inclusive.
Through these small examples, the message is clear: Teachers must remain in the driver’s seat. AI is like a trainee teacher’s aide – capable but needing supervision. By mastering the basics of AI vocabulary and being aware of issues like hallucinations and bias, you are empowered to use AI tools beneficially without surrendering your professional judgment. This balance upholds human agency in education, aligning with UNESCO’s principle of protecting human decision-making and autonomy in the age of AI
unesco.org
unesco.org
.
Interactive Quiz: Test Your AI Basics Knowledge
What best describes an AI “hallucination” in the context of an educational tool?
[[ ]] The AI gets confused and refuses to answer a question.
[[X]] The AI produces a very confident answer that includes made-up or incorrect information.
[[ ]] The AI generates an image instead of text in its answer unexpectedly.
[[ ]] The AI repeats exactly something it read in the training data.
A school uses an AI to identify students who might need extra help, based on past grades and attendance. What is a key step to ensure human agency and fairness?
[[X]] Have counselors or teachers review the AI’s suggestions and consider factors the AI might have missed (maintaining human judgment).
[[ ]] Let the AI’s list be final, since it’s data-driven and probably unbiased.
[[ ]] Not use AI at all, because any use of AI removes human agency.
[[X]] Check how the AI was trained – for example, ensure the data isn’t biased against any group of students – and monitor its recommendations over time for bias
unesco.org
unesco.org
.
Match the term to its definition (write the letter):
A. Model
B. Bias
C. Prompting
[[A]] A trained AI system that makes predictions or decisions based on learned patterns.
[[B]] An unfair trend in AI outputs that can discriminate, often due to skewed training data.
[[C]] The process of giving instructions or questions to an AI to get desired outputs.
(In order: A = Model, B = Bias, C = Prompting.)
True or False: A neural network “learns” by adjusting its weights based on training data, improving its performance over time.
[[X]] True
[[ ]] False
Which of these is an example of maintaining human agency when using an AI translation tool in class?
[[X]] Rani double-checks the AI’s translated technical terms in a circuit diagram explanation before giving it to students, to ensure accuracy and clarity.
[[ ]] Rani trusts the AI completely and hands out the translation without reading it, to save time.
[[X]] Rani allows students to use the tool but teaches them to verify the results and encourages them to ask if something seems off, keeping human judgment in the loop.
[[ ]] Rani prohibits any corrections to the AI output, assuming the AI knows best.
Great job! By answering these, you reinforce the concepts of AI basics. In summary, always remember: understand the tools you use (know the basics), and stay in charge (exercise human agency). Next, we’ll move on to exploring common AI tools and applications in education, connecting these concepts to real software you can use.
Reflection: How do you feel about using AI in your teaching after learning these basics? Write down one concern you have and one opportunity you see. Consider how maintaining human agency can address the concern.
Open answer: [[___]]
(Take a moment to reflect. There are no “right” answers here – this is for you to connect the module content with your own thoughts and context.)
<div style="text-align:center;"> <img src="https://github.com/OVGU-VET-TechEd/ASSET_UNESCO_Coinitiative/blob/main/media/UNESCO-UNEVOC_logo.png?raw=true" alt="UNEVOC Logo" width="80" style="margin:0 10px;"/> <img src="https://github.com/OVGU-VET-TechEd/ASSET_UNESCO_Coinitiative/blob/main/media/ASSET_icon.png?raw=true" alt="ASSET Logo" width="80" style="margin:0 10px;"/> <img src="https://github.com/OVGU-VET-TechEd/ASSET_UNESCO_Coinitiative/blob/main/media/hrdc_logo.png?raw=true" alt="HRDC Logo" width="80" style="margin:0 10px;"/> <img src="https://github.com/OVGU-VET-TechEd/ASSET_UNESCO_Coinitiative/blob/main/media/mitd_logo.png?raw=true" alt="MITD Logo" width="80" style="margin:0 10px;"/> <img src="https://github.com/OVGU-VET-TechEd/ASSET_UNESCO_Coinitiative/blob/main/media/HWK_Blume.png?raw=true" alt="HWK Blume Logo" width="80" style="margin:0 10px;"/> </div>